{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio_access_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"minio_secret_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\", \"<Your-MinIO-AccessKey>\"))\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"<Your-MinIO-SecretKey>\"))\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.getenv(\"ENDPOINT\", \"<Your-MinIO-Endpoint>\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "spark = (SparkSession.builder \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    # # .config(\"spark.jars.packages, \"org.apache.hudi:hudi-spark3.3-bundle_2.12:0.13.1\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n",
    "    # .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate())\n",
    "\n",
    "# Disable below line to see INFO logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "load_config(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_filepath = \"s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--date\", help=\"Date in format YYYY-MM-DD\", required=True)\n",
    "    # parser.add_argument(\"--source_files_pattern\", help=\"Source files pattern for the GH archive to process.\", required=True)\n",
    "    # parser.add_argument(\"--destination_files_pattern\", help=\"Destination files pattern for the GH archive to process.\", required=True)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # date = args.date\n",
    "\n",
    "    # read_filepath = args.source_files_pattern\n",
    "    # write_filepath = args.destination_files_pattern\n",
    "    # read_filepath=read_filepath.format(date)\n",
    "    # print(f\"date received: {date}\")\n",
    "\n",
    "\n",
    "    print(f\"read_filepath: {read_filepath}\")\n",
    "    df = spark.read.json(read_filepath)\n",
    "\n",
    "\n",
    "    allowed_events = [\n",
    "        \"PushEvent\",\n",
    "        \"ForkEvent\",\n",
    "        \"PublicEvent\",\n",
    "        \"WatchEvent\",\n",
    "        \"PullRequestEvent\",\n",
    "    ]\n",
    "\n",
    "    main_df = df.select(\n",
    "        F.col(\"id\").alias(\"event_id\"),\n",
    "        F.col(\"type\").alias(\"event_type\"),\n",
    "        F.to_timestamp( F.col(\"created_at\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\" ).alias(\"created_at\"),\n",
    "        F.col(\"repo.id\").alias(\"repository_id\"),\n",
    "        F.col(\"repo.name\").alias(\"repository_name\"),\n",
    "        F.col(\"repo.url\").alias(\"repository_url\"),\n",
    "        F.col(\"actor.id\").alias(\"user_id\"),\n",
    "        F.col(\"actor.login\").alias(\"user_name\"),\n",
    "        F.col(\"actor.url\").alias(\"user_url\"),\n",
    "        F.col(\"actor.avatar_url\").alias(\"user_avatar_url\"),\n",
    "        F.col(\"org.id\").alias(\"org_id\"),\n",
    "        F.col(\"org.login\").alias(\"org_name\"),\n",
    "        F.col(\"org.url\").alias(\"org_url\"),\n",
    "        F.col(\"org.avatar_url\").alias(\"org_avatar_url\"),\n",
    "        F.col(\"payload.push_id\").alias(\"push_id\"),\n",
    "        F.col(\"payload.distinct_size\").alias(\"number_of_commits\"),\n",
    "        F.col(\"payload.pull_request.base.repo.language\").alias(\"language\"),\n",
    "    ).filter(\n",
    "        F.col(\"type\").isin(allowed_events)\n",
    "    )\n",
    "\n",
    "    main_df = main_df.withColumn(\"year\", F.year(\"created_at\")) \\\n",
    "        .withColumn(\"month\", F.month(\"created_at\")) \\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"created_at\")) \\\n",
    "        .withColumn(\"hour\", F.hour(\"created_at\")) \\\n",
    "        .withColumn(\"minute\", F.minute(\"created_at\")) \\\n",
    "        .withColumn(\"second\", F.second(\"created_at\")) \\\n",
    "    \n",
    "    # add timestamp field\n",
    "    main_df = main_df.withColumn(\"ts\", F.unix_timestamp(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "    # add update date field \n",
    "    main_df = main_df.withColumn(\"updated_at\", F.current_timestamp())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For custom configuration visit \n",
    "# https://hudi.apache.org/docs/configurations/\n",
    "\n",
    "hudi_options = {\n",
    "    \"hoodie.table.name\": \"gh_archive\",\n",
    "    \"hoodie.datasource.write.storage.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.operation\": \"upsert\",\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"event_id\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"updated_at\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"year, month, day, hour\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\",\n",
    "    # \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    # \"hoodie.datasource.hive_sync.database\": \"<your_database_name>\",\n",
    "    # \"hoodie.datasource.hive_sync.table\": \"<your_table_name>\",\n",
    "    # \"hoodie.datasource.hive_sync.partition_fields\": \"<your_partitionkey_field>\",\n",
    "    # \"hoodie.datasource.hive_sync.partition_extractor_class\": \"org.apache.hudi.hive.MultiPartKeysValueExtractor\",\n",
    "    # \"hoodie.datasource.hive_sync.use_jdbc\": \"false\",\n",
    "    # \"hoodie.datasource.hive_sync.mode\": \"hms\"\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2\n",
    "}\n",
    "\n",
    "\n",
    "main_df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"append\"). \\\n",
    "    save(\"s3a://gh-archive-data-curated/hudi/gh_archive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df = spark.read.format(\"hudi\").load(\"s3a://gh-archive-data-curated/hudi/gh_archive/\")\n",
    "gh_archive_df.createOrReplaceTempView(\"gh_archive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

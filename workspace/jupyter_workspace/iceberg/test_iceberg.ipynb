{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding iceberg configs\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Use Iceberg with Spark\n",
    "    .set(\"spark.sql.defaultCatalog\", \"demo\") # Name of the Iceberg catalog\n",
    "    .set(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    # .set(\"spark.sql.catalog.demo.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\")\n",
    "    .set(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.demo.warehouse\", \"s3a://gh-archive-data-curated/iceberg\")\n",
    "    .set(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\")\n",
    "    .set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "    .set(\"spark.sql.catalog.demo.type\", \"hadoop\") # Iceberg catalog type\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    "    .set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") # Use \"dynamic\"   \n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Disable below line to see INFO logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio_access_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"minio_secret_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "load_config(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_filepath = \"s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--date\", help=\"Date in format YYYY-MM-DD\", required=True)\n",
    "    # parser.add_argument(\"--source_files_pattern\", help=\"Source files pattern for the GH archive to process.\", required=True)\n",
    "    # parser.add_argument(\"--destination_files_pattern\", help=\"Destination files pattern for the GH archive to process.\", required=True)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # date = args.date\n",
    "\n",
    "    # read_filepath = args.source_files_pattern\n",
    "    # write_filepath = args.destination_files_pattern\n",
    "    # read_filepath=read_filepath.format(date)\n",
    "    # print(f\"date received: {date}\")\n",
    "\n",
    "\n",
    "    print(f\"read_filepath: {read_filepath}\")\n",
    "    df = spark.read.json(read_filepath)\n",
    "\n",
    "\n",
    "    allowed_events = [\n",
    "        \"PushEvent\",\n",
    "        \"ForkEvent\",\n",
    "        \"PublicEvent\",\n",
    "        \"WatchEvent\",\n",
    "        \"PullRequestEvent\",\n",
    "    ]\n",
    "\n",
    "    main_df = df.select(\n",
    "        F.col(\"id\").alias(\"event_id\"),\n",
    "        F.col(\"type\").alias(\"event_type\"),\n",
    "        F.to_timestamp( F.col(\"created_at\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\" ).alias(\"created_at\"),\n",
    "        F.col(\"repo.id\").alias(\"repository_id\"),\n",
    "        F.col(\"repo.name\").alias(\"repository_name\"),\n",
    "        F.col(\"repo.url\").alias(\"repository_url\"),\n",
    "        F.col(\"actor.id\").alias(\"user_id\"),\n",
    "        F.col(\"actor.login\").alias(\"user_name\"),\n",
    "        F.col(\"actor.url\").alias(\"user_url\"),\n",
    "        F.col(\"actor.avatar_url\").alias(\"user_avatar_url\"),\n",
    "        F.col(\"org.id\").alias(\"org_id\"),\n",
    "        F.col(\"org.login\").alias(\"org_name\"),\n",
    "        F.col(\"org.url\").alias(\"org_url\"),\n",
    "        F.col(\"org.avatar_url\").alias(\"org_avatar_url\"),\n",
    "        F.col(\"payload.push_id\").alias(\"push_id\"),\n",
    "        F.col(\"payload.distinct_size\").alias(\"number_of_commits\"),\n",
    "        F.col(\"payload.pull_request.base.repo.language\").alias(\"language\"),\n",
    "    ).filter(\n",
    "        F.col(\"type\").isin(allowed_events)\n",
    "    )\n",
    "\n",
    "    main_df = main_df.withColumn(\"year\", F.year(\"created_at\")) \\\n",
    "        .withColumn(\"month\", F.month(\"created_at\")) \\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"created_at\")) \\\n",
    "        .withColumn(\"hour\", F.hour(\"created_at\")) \\\n",
    "        .withColumn(\"minute\", F.minute(\"created_at\")) \\\n",
    "        .withColumn(\"second\", F.second(\"created_at\")) \\\n",
    "    \n",
    "    # add timestamp field\n",
    "    main_df = main_df.withColumn(\"ts\", F.unix_timestamp(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "    # add update date field \n",
    "    main_df = main_df.withColumn(\"updated_at\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.write.mode(\"append\").partitionBy(\"year\", \"month\", \"day\", \"hour\").saveAsTable(\"gh.gh_archive_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df = spark.read.format(\"iceberg\").load(\"gh.gh_archive_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "24/02/10 23:15:29 INFO SparkContext: Running Spark version 3.3.0-amzn-1\n",
      "24/02/10 23:15:29 INFO ResourceUtils: ==============================================================\n",
      "24/02/10 23:15:29 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/02/10 23:15:29 INFO ResourceUtils: ==============================================================\n",
      "24/02/10 23:15:29 INFO SparkContext: Submitted application: pyspark-shell\n",
      "24/02/10 23:15:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/02/10 23:15:29 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/02/10 23:15:29 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/02/10 23:15:29 INFO SecurityManager: Changing view acls to: glue_user\n",
      "24/02/10 23:15:29 INFO SecurityManager: Changing modify acls to: glue_user\n",
      "24/02/10 23:15:29 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/10 23:15:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/10 23:15:29 INFO SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(glue_user); groups with view permissions: Set(); users  with modify permissions: Set(glue_user); groups with modify permissions: Set()\n",
      "24/02/10 23:15:30 INFO Utils: Successfully started service 'sparkDriver' on port 41875.\n",
      "24/02/10 23:15:30 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/02/10 23:15:30 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/02/10 23:15:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/02/10 23:15:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/02/10 23:15:30 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/02/10 23:15:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b22770af-da64-49e4-b5e3-b98b700ae352\n",
      "24/02/10 23:15:30 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "24/02/10 23:15:30 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/02/10 23:15:30 INFO SubResultCacheManager: Sub-result caches are disabled.\n",
      "24/02/10 23:15:30 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/02/10 23:15:30 INFO Executor: Starting executor ID driver on host 931652ef741a\n",
      "24/02/10 23:15:30 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/home/glue_user/aws-glue-libs/datalake-connectors/iceberg-1.0.0/*,file:/home/glue_user/aws-glue-libs/datalake-connectors/hudi-0.12.1/*,file:/home/glue_user/spark/jars/*,file:/home/glue_user/aws-glue-libs/jars/*,file:/home/glue_user/workspace/jupyter_workspace/iceberg/*'\n",
      "24/02/10 23:15:30 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37735.\n",
      "24/02/10 23:15:30 INFO NettyBlockTransferService: Server created on 931652ef741a:37735\n",
      "24/02/10 23:15:30 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/02/10 23:15:30 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 931652ef741a, 37735, None)\n",
      "24/02/10 23:15:30 INFO BlockManagerMasterEndpoint: Registering block manager 931652ef741a:37735 with 366.3 MiB RAM, BlockManagerId(driver, 931652ef741a, 37735, None)\n",
      "24/02/10 23:15:30 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 931652ef741a, 37735, None)\n",
      "24/02/10 23:15:30 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 931652ef741a, 37735, None)\n",
      "24/02/10 23:15:31 INFO SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/local-1707606930763.inprogress\n"
     ]
    }
   ],
   "source": [
    "# adding iceberg configs\n",
    "conf = (\n",
    "    SparkConf()\n",
    "    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") # Use Iceberg with Spark\n",
    "    .set(\"spark.sql.defaultCatalog\", \"demo\") # Name of the Iceberg catalog\n",
    "    .set(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .set(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "    .set(\"spark.sql.catalog.demo.warehouse\", \"s3a://gh-archive-data-curated/iceberg\")\n",
    "    .set(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\")\n",
    "    .set(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "    .set(\"spark.sql.catalog.demo.type\", \"hadoop\") # Iceberg catalog type\n",
    "    .set(\"spark.executor.heartbeatInterval\", \"300000\")\n",
    "    .set(\"spark.network.timeout\", \"400000\")\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Disable below line to see INFO logs\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio_access_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"minio_secret_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "load_config(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_filepath = \"s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_filepath: s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=4/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--date\", help=\"Date in format YYYY-MM-DD\", required=True)\n",
    "    # parser.add_argument(\"--source_files_pattern\", help=\"Source files pattern for the GH archive to process.\", required=True)\n",
    "    # parser.add_argument(\"--destination_files_pattern\", help=\"Destination files pattern for the GH archive to process.\", required=True)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # date = args.date\n",
    "\n",
    "    # read_filepath = args.source_files_pattern\n",
    "    # write_filepath = args.destination_files_pattern\n",
    "    # read_filepath=read_filepath.format(date)\n",
    "    # print(f\"date received: {date}\")\n",
    "\n",
    "\n",
    "    print(f\"read_filepath: {read_filepath}\")\n",
    "    df = spark.read.json(read_filepath)\n",
    "\n",
    "\n",
    "    allowed_events = [\n",
    "        \"PushEvent\",\n",
    "        \"ForkEvent\",\n",
    "        \"PublicEvent\",\n",
    "        \"WatchEvent\",\n",
    "        \"PullRequestEvent\",\n",
    "    ]\n",
    "\n",
    "    main_df = df.select(\n",
    "        F.col(\"id\").alias(\"event_id\"),\n",
    "        F.col(\"type\").alias(\"event_type\"),\n",
    "        F.to_timestamp( F.col(\"created_at\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\" ).alias(\"created_at\"),\n",
    "        F.col(\"repo.id\").alias(\"repository_id\"),\n",
    "        F.col(\"repo.name\").alias(\"repository_name\"),\n",
    "        F.col(\"repo.url\").alias(\"repository_url\"),\n",
    "        F.col(\"actor.id\").alias(\"user_id\"),\n",
    "        F.col(\"actor.login\").alias(\"user_name\"),\n",
    "        F.col(\"actor.url\").alias(\"user_url\"),\n",
    "        F.col(\"actor.avatar_url\").alias(\"user_avatar_url\"),\n",
    "        F.col(\"org.id\").alias(\"org_id\"),\n",
    "        F.col(\"org.login\").alias(\"org_name\"),\n",
    "        F.col(\"org.url\").alias(\"org_url\"),\n",
    "        F.col(\"org.avatar_url\").alias(\"org_avatar_url\"),\n",
    "        F.col(\"payload.push_id\").alias(\"push_id\"),\n",
    "        F.col(\"payload.distinct_size\").alias(\"number_of_commits\"),\n",
    "        F.col(\"payload.pull_request.base.repo.language\").alias(\"language\"),\n",
    "    ).filter(\n",
    "        F.col(\"type\").isin(allowed_events)\n",
    "    )\n",
    "\n",
    "    main_df = main_df.withColumn(\"year\", F.year(\"created_at\")) \\\n",
    "        .withColumn(\"month\", F.month(\"created_at\")) \\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"created_at\")) \\\n",
    "        .withColumn(\"hour\", F.hour(\"created_at\")) \\\n",
    "        .withColumn(\"minute\", F.minute(\"created_at\")) \\\n",
    "        .withColumn(\"second\", F.second(\"created_at\")) \\\n",
    "    \n",
    "    # add timestamp field\n",
    "    main_df = main_df.withColumn(\"ts\", F.unix_timestamp(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "\n",
    "    # write the DataFrame to GCS partitioned by year, month, and day and bucketed by hour and minute\n",
    "    # date = date.replace(\"-\", \"\")\n",
    "\n",
    "    # main_df.write \\\n",
    "    # .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    # .bucketBy(24, \"hour\") \\\n",
    "    # .sortBy(\"hour\", \"minute\") \\\n",
    "    # .option(\"path\", write_filepath) \\\n",
    "    # .option(\"header\", True) \\\n",
    "    # .mode(\"append\") \\\n",
    "    # .saveAsTable(f\"table{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "|   event_id|      event_type|         created_at|repository_id|     repository_name|      repository_url|  user_id|          user_name|            user_url|     user_avatar_url|  org_id|           org_name|             org_url|      org_avatar_url|    push_id|number_of_commits|  language|year|month|day|hour|minute|second|        ts|\n",
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "|26164467849|       PushEvent|2023-01-01 04:00:00|    575893918|Rolleander/yoyo_m...|https://api.githu...|  9052189|         Rolleander|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893284|                0|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467852|       PushEvent|2023-01-01 04:00:00|    400167946| liurenjie520/yujing|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893281|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467857|       PushEvent|2023-01-01 04:00:00|    392024283|DhyellTorres/Dhye...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893282|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467859|PullRequestEvent|2023-01-01 04:00:00|    155222419|SphericalPotatoIn...|https://api.githu...| 49699333|    dependabot[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|       null|             null|JavaScript|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467860|       PushEvent|2023-01-01 04:00:00|    372251919|       Searge/Searge|https://api.githu...|  2125311|             Searge|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893286|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467862|       PushEvent|2023-01-01 04:00:00|    583891383|junron/fart-penet...|https://api.githu...| 26194273|             junron|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893291|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467866|       PushEvent|2023-01-01 04:00:00|    583107991|Phantasm30/Kinkie...|https://api.githu...| 89787145|         Phantasm30|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893292|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467868|       PushEvent|2023-01-01 04:00:00|    132043451|KhronosGroup/Vulk...|https://api.githu...|115671160|     spencer-lunarg|https://api.githu...|https://avatars.g...| 1608701|       KhronosGroup|https://api.githu...|https://avatars.g...|12147893288|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467870|       PushEvent|2023-01-01 04:00:00|    582117755|MinekiYoshino/Gre...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893296|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467874|       PushEvent|2023-01-01 04:00:00|    583532818|arihant2math/ftc-api|https://api.githu...| 58895652|       arihant2math|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893299|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467875|      WatchEvent|2023-01-01 04:00:00|    258028021|Polydile/dile-com...|https://api.githu...| 38342390|           andy3520|https://api.githu...|https://avatars.g...|34108498|           Polydile|https://api.githu...|https://avatars.g...|       null|             null|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467878|       PushEvent|2023-01-01 04:00:00|    575893918|Rolleander/yoyo_m...|https://api.githu...|  9052189|         Rolleander|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893298|                0|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467880|       PushEvent|2023-01-01 04:00:00|    563002031|Tianyijian/LeetCo...|https://api.githu...| 23497428|         Tianyijian|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893304|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467883|       ForkEvent|2023-01-01 04:00:00|    390206868|Gladiators-Projec...|https://api.githu...| 53547435|        samihosni01|https://api.githu...|https://avatars.g...|87497333|Gladiators-Projects|https://api.githu...|https://avatars.g...|       null|             null|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467887|       PushEvent|2023-01-01 04:00:00|     18790370|  sminnee/blockworld|https://api.githu...| 49699333|    dependabot[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893306|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467884|       PushEvent|2023-01-01 04:00:00|    458391225|DeivSoares/DeivSo...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893311|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467885|       PushEvent|2023-01-01 04:00:00|    578304300|kiwookim/lets_mee...|https://api.githu...| 95503033|           kiwookim|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893305|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467888|       PushEvent|2023-01-01 04:00:00|    584035821|tfutch87/frontend...|https://api.githu...| 29153556|           tfutch87|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893301|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467890|       PushEvent|2023-01-01 04:00:00|    539826765|  ifuncool/ireadlife|https://api.githu...| 54922142|           ifuncool|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893308|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "|26164467895|       PushEvent|2023-01-01 04:00:00|    582512370|       B4kedBr3ad/yo|https://api.githu...|110211701|         B4kedBr3ad|https://api.githu...|https://avatars.g...|    null|               null|                null|                null|12147893310|                1|      null|2023|    1|  1|   4|     0|     0|1672545600|\n",
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+-------------------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "main_df.write.mode(\"overwrite\").saveAsTable(\"gh.gh_archive_data\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

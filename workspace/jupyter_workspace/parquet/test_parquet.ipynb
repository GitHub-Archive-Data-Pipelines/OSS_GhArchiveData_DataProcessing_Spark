{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/glue_user/aws-glue-libs/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Reload4jLoggerFactory]\n",
      "24/02/10 20:03:06 INFO SparkContext: Running Spark version 3.3.0-amzn-1\n",
      "24/02/10 20:03:06 INFO ResourceUtils: ==============================================================\n",
      "24/02/10 20:03:06 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/02/10 20:03:06 INFO ResourceUtils: ==============================================================\n",
      "24/02/10 20:03:06 INFO SparkContext: Submitted application: pyspark-shell\n",
      "24/02/10 20:03:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/02/10 20:03:06 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/02/10 20:03:06 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/02/10 20:03:07 INFO SecurityManager: Changing view acls to: glue_user\n",
      "24/02/10 20:03:07 INFO SecurityManager: Changing modify acls to: glue_user\n",
      "24/02/10 20:03:07 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/10 20:03:07 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/10 20:03:07 INFO SecurityManager: SecurityManager: authentication enabled; ui acls disabled; users  with view permissions: Set(glue_user); groups with view permissions: Set(); users  with modify permissions: Set(glue_user); groups with modify permissions: Set()\n",
      "24/02/10 20:03:07 INFO Utils: Successfully started service 'sparkDriver' on port 34407.\n",
      "24/02/10 20:03:07 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/02/10 20:03:07 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/02/10 20:03:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/02/10 20:03:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/02/10 20:03:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/02/10 20:03:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9605f0eb-a508-46a9-9700-2e663a0ef766\n",
      "24/02/10 20:03:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "24/02/10 20:03:07 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/02/10 20:03:07 INFO SubResultCacheManager: Sub-result caches are disabled.\n",
      "24/02/10 20:03:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/02/10 20:03:08 INFO Executor: Starting executor ID driver on host 931652ef741a\n",
      "24/02/10 20:03:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): 'file:/home/glue_user/aws-glue-libs/datalake-connectors/iceberg-1.0.0/*,file:/home/glue_user/aws-glue-libs/datalake-connectors/hudi-0.12.1/*,file:/home/glue_user/spark/jars/*,file:/home/glue_user/aws-glue-libs/jars/*,file:/home/glue_user/workspace/jupyter_workspace/parquet/*'\n",
      "24/02/10 20:03:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39869.\n",
      "24/02/10 20:03:08 INFO NettyBlockTransferService: Server created on 931652ef741a:39869\n",
      "24/02/10 20:03:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/02/10 20:03:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 931652ef741a, 39869, None)\n",
      "24/02/10 20:03:08 INFO BlockManagerMasterEndpoint: Registering block manager 931652ef741a:39869 with 366.3 MiB RAM, BlockManagerId(driver, 931652ef741a, 39869, None)\n",
      "24/02/10 20:03:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 931652ef741a, 39869, None)\n",
      "24/02/10 20:03:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 931652ef741a, 39869, None)\n",
      "24/02/10 20:03:08 INFO SingleEventLogFileWriter: Logging events to file:/tmp/spark-events/local-1707595387990.inprogress\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_config(spark_context: SparkContext):\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"minio_access_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",\"minio_secret_key\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.getenv(\"AWS_ACCESS_KEY_ID\", \"<Your-MinIO-AccessKey>\"))\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\",os.getenv(\"AWS_SECRET_ACCESS_KEY\", \"<Your-MinIO-SecretKey>\"))\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.getenv(\"ENDPOINT\", \"<Your-MinIO-Endpoint>\"))\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.attempts.maximum\", \"1\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.establish.timeout\", \"5000\")\n",
    "    # spark_context._jsc.hadoopConfiguration().set(\"fs.s3a.connection.timeout\", \"10000\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "load_config(spark.sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_filepath = \"s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=5/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_filepath: s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 20:03:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/02/10 20:03:09 INFO SharedState: Warehouse path is 'file:/home/glue_user/workspace/jupyter_workspace/parquet/spark-warehouse'.\n",
      "24/02/10 20:03:10 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/02/10 20:03:10 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "24/02/10 20:03:10 INFO MetricsSystemImpl: s3a-file-system metrics system started\n",
      "24/02/10 20:03:11 WARN ApacheUtils: NoSuchMethodException was thrown when disabling normalizeUri. This indicates you are using an old version (< 4.5.8) of Apache http client. It is recommended to use http client version >= 4.5.9 to avoid the breaking change introduced in apache client 4.5.7 and the latency in exception handling. See https://github.com/aws/aws-sdk-java/issues/1919 for more information\n",
      "24/02/10 20:03:11 INFO InMemoryFileIndex: It took 57 ms to list leaf files for 1 paths.\n",
      "24/02/10 20:03:12 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.\n",
      "24/02/10 20:03:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/02/10 20:03:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/02/10 20:03:15 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "24/02/10 20:03:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 498.5 KiB, free 365.8 MiB)\n",
      "24/02/10 20:03:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 52.1 KiB, free 365.8 MiB)\n",
      "24/02/10 20:03:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 931652ef741a:39869 (size: 52.1 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:15 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:03:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8812078 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\n",
      "24/02/10 20:03:15 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\n",
      "24/02/10 20:03:15 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/02/10 20:03:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 17.7 KiB, free 365.7 MiB)\n",
      "24/02/10 20:03:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.6 KiB, free 365.7 MiB)\n",
      "24/02/10 20:03:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 931652ef741a:39869 (size: 8.6 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1570\n",
      "24/02/10 20:03:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/10 20:03:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/02/10 20:03:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (931652ef741a, executor driver, partition 0, PROCESS_LOCAL, 5073 bytes) taskResourceAssignments Map()\n",
      "24/02/10 20:03:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/02/10 20:03:16 INFO FileScanRDD: TID: 0 - Reading current file: path: s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=5/2023-01-01-5.json.gz, range: 0-66302326, partition values: [empty row], isDataPresent: false, eTag: null\n",
      "24/02/10 20:03:16 INFO CodeGenerator: Code generated in 153.827976 ms\n",
      "24/02/10 20:03:16 INFO ZlibFactory: Successfully loaded & initialized native-zlib library\n",
      "24/02/10 20:03:16 INFO CodecPool: Got brand-new decompressor [.gz]\n",
      "24/02/10 20:03:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 28570 bytes result sent to driver\n",
      "24/02/10 20:03:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6601 ms on 931652ef741a (executor driver) (1/1)\n",
      "24/02/10 20:03:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/02/10 20:03:22 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 6.807 s\n",
      "24/02/10 20:03:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/10 20:03:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/02/10 20:03:22 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 6.864862 s\n",
      "24/02/10 20:03:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--date\", help=\"Date in format YYYY-MM-DD\", required=True)\n",
    "    # parser.add_argument(\"--source_files_pattern\", help=\"Source files pattern for the GH archive to process.\", required=True)\n",
    "    # parser.add_argument(\"--destination_files_pattern\", help=\"Destination files pattern for the GH archive to process.\", required=True)\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    # date = args.date\n",
    "\n",
    "    # read_filepath = args.source_files_pattern\n",
    "    # write_filepath = args.destination_files_pattern\n",
    "    # read_filepath=read_filepath.format(date)\n",
    "    # print(f\"date received: {date}\")\n",
    "\n",
    "\n",
    "    print(f\"read_filepath: {read_filepath}\")\n",
    "    df = spark.read.json(read_filepath)\n",
    "\n",
    "\n",
    "    allowed_events = [\n",
    "        \"PushEvent\",\n",
    "        \"ForkEvent\",\n",
    "        \"PublicEvent\",\n",
    "        \"WatchEvent\",\n",
    "        \"PullRequestEvent\",\n",
    "    ]\n",
    "\n",
    "    main_df = df.select(\n",
    "        F.col(\"id\").alias(\"event_id\"),\n",
    "        F.col(\"type\").alias(\"event_type\"),\n",
    "        F.to_timestamp( F.col(\"created_at\"), \"yyyy-MM-dd'T'HH:mm:ss'Z'\" ).alias(\"created_at\"),\n",
    "        F.col(\"repo.id\").alias(\"repository_id\"),\n",
    "        F.col(\"repo.name\").alias(\"repository_name\"),\n",
    "        F.col(\"repo.url\").alias(\"repository_url\"),\n",
    "        F.col(\"actor.id\").alias(\"user_id\"),\n",
    "        F.col(\"actor.login\").alias(\"user_name\"),\n",
    "        F.col(\"actor.url\").alias(\"user_url\"),\n",
    "        F.col(\"actor.avatar_url\").alias(\"user_avatar_url\"),\n",
    "        F.col(\"org.id\").alias(\"org_id\"),\n",
    "        F.col(\"org.login\").alias(\"org_name\"),\n",
    "        F.col(\"org.url\").alias(\"org_url\"),\n",
    "        F.col(\"org.avatar_url\").alias(\"org_avatar_url\"),\n",
    "        F.col(\"payload.push_id\").alias(\"push_id\"),\n",
    "        F.col(\"payload.distinct_size\").alias(\"number_of_commits\"),\n",
    "        F.col(\"payload.pull_request.base.repo.language\").alias(\"language\"),\n",
    "    ).filter(\n",
    "        F.col(\"type\").isin(allowed_events)\n",
    "    )\n",
    "\n",
    "    main_df = main_df.withColumn(\"year\", F.year(\"created_at\")) \\\n",
    "        .withColumn(\"month\", F.month(\"created_at\")) \\\n",
    "        .withColumn(\"day\", F.dayofmonth(\"created_at\")) \\\n",
    "        .withColumn(\"hour\", F.hour(\"created_at\")) \\\n",
    "        .withColumn(\"minute\", F.minute(\"created_at\")) \\\n",
    "        .withColumn(\"second\", F.second(\"created_at\")) \\\n",
    "    \n",
    "    # add timestamp field\n",
    "    main_df = main_df.withColumn(\"ts\", F.unix_timestamp(\"created_at\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "\n",
    "    # write the DataFrame to GCS partitioned by year, month, and day and bucketed by hour and minute\n",
    "    # date = date.replace(\"-\", \"\")\n",
    "\n",
    "    # main_df.write \\\n",
    "    # .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    # .bucketBy(24, \"hour\") \\\n",
    "    # .sortBy(\"hour\", \"minute\") \\\n",
    "    # .option(\"path\", write_filepath) \\\n",
    "    # .option(\"header\", True) \\\n",
    "    # .mode(\"append\") \\\n",
    "    # .saveAsTable(f\"table{date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 20:03:23 INFO FileSourceStrategy: Pushed Filters: In(type, [ForkEvent,PublicEvent,PullRequestEvent,PushEvent,WatchEvent])\n",
      "24/02/10 20:03:23 INFO FileSourceStrategy: Post-Scan Filters: type#15 IN (PushEvent,ForkEvent,PublicEvent,WatchEvent,PullRequestEvent)\n",
      "24/02/10 20:03:23 INFO FileSourceStrategy: Output Data Schema: struct<actor: struct<avatar_url: string, display_login: string, gravatar_id: string, id: bigint, login: string ... 1 more field>, created_at: string, id: string, org: struct<avatar_url: string, gravatar_id: string, id: bigint, login: string, url: string ... 3 more fields>, payload: struct<action: string, before: string, comment: struct<_links: struct<html: struct<href: string>, pull_request: struct<href: string>, self: struct<href: string> ... 1 more fields>, author_association: string, body: string, commit_id: string, created_at: string ... 26 more fields>, commits: array<struct<author:struct<email:string,name:string>,distinct:boolean,message:string,sha:string,url:string>>, description: string ... 19 more fields> ... 5 more fields>\n",
      "24/02/10 20:03:23 INFO CodeGenerator: Code generated in 69.012385 ms\n",
      "24/02/10 20:03:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 498.4 KiB, free 365.2 MiB)\n",
      "24/02/10 20:03:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 52.1 KiB, free 365.2 MiB)\n",
      "24/02/10 20:03:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 931652ef741a:39869 (size: 52.1 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:23 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:03:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8812078 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\n",
      "24/02/10 20:03:23 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\n",
      "24/02/10 20:03:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:03:23 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/02/10 20:03:23 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/02/10 20:03:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/10 20:03:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/10 20:03:23 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/02/10 20:03:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 931652ef741a:39869 in memory (size: 52.1 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 931652ef741a:39869 in memory (size: 8.6 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 81.5 KiB, free 365.7 MiB)\n",
      "24/02/10 20:03:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 24.7 KiB, free 365.7 MiB)\n",
      "24/02/10 20:03:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 931652ef741a:39869 (size: 24.7 KiB, free: 366.2 MiB)\n",
      "24/02/10 20:03:24 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1570\n",
      "24/02/10 20:03:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/10 20:03:24 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/02/10 20:03:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (931652ef741a, executor driver, partition 0, PROCESS_LOCAL, 5073 bytes) taskResourceAssignments Map()\n",
      "24/02/10 20:03:24 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/02/10 20:03:24 INFO CodeGenerator: Code generated in 45.929842 ms\n",
      "24/02/10 20:03:24 INFO CodeGenerator: Code generated in 49.198632 ms\n",
      "24/02/10 20:03:24 INFO FileScanRDD: TID: 1 - Reading current file: path: s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=5/2023-01-01-5.json.gz, range: 0-66302326, partition values: [empty row], isDataPresent: false, eTag: null\n",
      "24/02/10 20:03:24 INFO CodeGenerator: Code generated in 384.871533 ms0 + 1) / 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "|   event_id|      event_type|         created_at|repository_id|     repository_name|      repository_url|  user_id|          user_name|            user_url|     user_avatar_url|  org_id|  org_name|             org_url|      org_avatar_url|    push_id|number_of_commits|  language|year|month|day|hour|minute|second|        ts|\n",
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "|26164710177|       PushEvent|2023-01-01 05:00:00|    516160021|2288256-sub/22882...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046619|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710179|       PushEvent|2023-01-01 05:00:00|    584049730|om0852/Netflix-clone|https://api.githu...|103877025|             om0852|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046616|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710180|       PushEvent|2023-01-01 05:00:00|    455285529|   Re1st-01/Re1st-01|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046620|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710183|       PushEvent|2023-01-01 05:00:00|    584049797|linktotheart/open...|https://api.githu...| 44489787|       linktotheart|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046621|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710186|      WatchEvent|2023-01-01 05:00:00|     20893207|     primer/octicons|https://api.githu...| 55990159|           golu7679|https://api.githu...|https://avatars.g...| 7143434|    primer|https://api.githu...|https://avatars.g...|       null|             null|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710188|       PushEvent|2023-01-01 05:00:00|    456117140|svilaa/websites-s...|https://api.githu...|  5521724|             svilaa|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046629|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710190|       PushEvent|2023-01-01 05:00:00|    584049780|Subhrajyoti2608/c...|https://api.githu...| 86191223|    Subhrajyoti2608|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046624|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710193|PullRequestEvent|2023-01-01 05:00:00|    173735083|laxman2/Angular7D...|https://api.githu...| 49699333|    dependabot[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|       null|             null|TypeScript|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710194|       PushEvent|2023-01-01 05:00:00|    544008304|XDream8/Wolfs-Kis...|https://api.githu...| 62709801|            XDream8|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046632|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710195|PullRequestEvent|2023-01-01 05:00:00|    524431029| reaitten/register-1|https://api.githu...| 39814207|          pull[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|       null|             null|JavaScript|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710196|       PushEvent|2023-01-01 05:00:00|    188227327|podium-lib/fastif...|https://api.githu...| 29139614|      renovate[bot]|https://api.githu...|https://avatars.g...|40788392|podium-lib|https://api.githu...|https://avatars.g...|12148046622|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710207|       PushEvent|2023-01-01 05:00:00|    531743944|silvio-santos/sil...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046645|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710208|       PushEvent|2023-01-01 05:00:00|    582116638|MaihaMemoto/Commi...|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046640|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710210|       PushEvent|2023-01-01 05:00:00|    575382558| SvkCreations/shopme|https://api.githu...| 95845696|       SvkCreations|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046642|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710216|       PushEvent|2023-01-01 05:00:00|    577965332|alexhyer/algo_tra...|https://api.githu...| 35442826|           alexhyer|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046647|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710219|       PushEvent|2023-01-01 05:00:00|    284236387| CodexLink/CodexLink|https://api.githu...| 41898282|github-actions[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046646|                1|      null|2023|    1|  1|   5|     0|     0|1672549200|\n",
      "|26164710221|       PushEvent|2023-01-01 05:00:01|    555503987|ShreyashSomvanshi...|https://api.githu...| 91454227|  ShreyashSomvanshi|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|12148046649|                1|      null|2023|    1|  1|   5|     0|     1|1672549201|\n",
      "|26164710230|      WatchEvent|2023-01-01 05:00:01|     69495170|     fastify/fastify|https://api.githu...| 46078454|         AthithyanR|https://api.githu...|https://avatars.g...|24939410|   fastify|https://api.githu...|https://avatars.g...|       null|             null|      null|2023|    1|  1|   5|     0|     1|1672549201|\n",
      "|26164710240|PullRequestEvent|2023-01-01 05:00:01|    173694014|haseeAmarathunga/...|https://api.githu...| 49699333|    dependabot[bot]|https://api.githu...|https://avatars.g...|    null|      null|                null|                null|       null|             null|TypeScript|2023|    1|  1|   5|     0|     1|1672549201|\n",
      "|26164710246|       PushEvent|2023-01-01 05:00:01|    512850650|bitcoinsit/CloudS...|https://api.githu...|109098696|   avanhoveln-BITCO|https://api.githu...|https://avatars.g...|85753025|bitcoinsit|https://api.githu...|https://avatars.g...|12148046665|                1|      null|2023|    1|  1|   5|     0|     1|1672549201|\n",
      "+-----------+----------------+-------------------+-------------+--------------------+--------------------+---------+-------------------+--------------------+--------------------+--------+----------+--------------------+--------------------+-----------+-----------------+----------+----+-----+---+----+------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 20:03:24 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5805 bytes result sent to driver\n",
      "24/02/10 20:03:24 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 807 ms on 931652ef741a (executor driver) (1/1)\n",
      "24/02/10 20:03:24 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/02/10 20:03:24 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.869 s\n",
      "24/02/10 20:03:24 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/10 20:03:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/02/10 20:03:24 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.874669 s\n",
      "24/02/10 20:03:24 INFO CodeGenerator: Code generated in 23.23986 ms             \n"
     ]
    }
   ],
   "source": [
    "main_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 20:05:37 INFO FileSourceStrategy: Pushed Filters: In(type, [ForkEvent,PublicEvent,PullRequestEvent,PushEvent,WatchEvent])\n",
      "24/02/10 20:05:37 INFO FileSourceStrategy: Post-Scan Filters: type#15 IN (PushEvent,ForkEvent,PublicEvent,WatchEvent,PullRequestEvent)\n",
      "24/02/10 20:05:37 INFO FileSourceStrategy: Output Data Schema: struct<actor: struct<avatar_url: string, display_login: string, gravatar_id: string, id: bigint, login: string ... 1 more field>, created_at: string, id: string, org: struct<avatar_url: string, gravatar_id: string, id: bigint, login: string, url: string ... 3 more fields>, payload: struct<action: string, before: string, comment: struct<_links: struct<html: struct<href: string>, pull_request: struct<href: string>, self: struct<href: string> ... 1 more fields>, author_association: string, body: string, commit_id: string, created_at: string ... 26 more fields>, commits: array<struct<author:struct<email:string,name:string>,distinct:boolean,message:string,sha:string,url:string>>, description: string ... 19 more fields> ... 5 more fields>\n",
      "24/02/10 20:05:38 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/02/10 20:05:38 INFO SQLConfCommitterProvider: Getting user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/02/10 20:05:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "24/02/10 20:05:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/02/10 20:05:38 INFO SQLConfCommitterProvider: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/02/10 20:05:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 498.4 KiB, free 364.2 MiB)\n",
      "24/02/10 20:05:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 52.1 KiB, free 364.2 MiB)\n",
      "24/02/10 20:05:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 931652ef741a:39869 (size: 52.1 KiB, free: 366.0 MiB)\n",
      "24/02/10 20:05:38 INFO SparkContext: Created broadcast 6 from save at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:05:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 8812078 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\n",
      "24/02/10 20:05:38 INFO FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: Vector((1 fileSplits,1))\n",
      "24/02/10 20:05:38 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/02/10 20:05:38 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 374.1 KiB, free 363.8 MiB)\n",
      "24/02/10 20:05:38 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 131.2 KiB, free 363.7 MiB)\n",
      "24/02/10 20:05:38 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 931652ef741a:39869 (size: 131.2 KiB, free: 365.9 MiB)\n",
      "24/02/10 20:05:38 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1570\n",
      "24/02/10 20:05:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[20] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/02/10 20:05:38 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/02/10 20:05:38 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (931652ef741a, executor driver, partition 0, PROCESS_LOCAL, 5073 bytes) taskResourceAssignments Map()\n",
      "24/02/10 20:05:38 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "24/02/10 20:05:38 INFO SQLConfCommitterProvider: Getting user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/02/10 20:05:38 INFO FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "24/02/10 20:05:38 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/02/10 20:05:38 INFO SQLConfCommitterProvider: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/02/10 20:05:38 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"event_id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"event_type\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"created_at\",\n",
      "    \"type\" : \"timestamp\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"repository_id\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"repository_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"repository_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"user_id\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"user_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"user_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"user_avatar_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"org_id\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"org_name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"org_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"org_avatar_url\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"push_id\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"number_of_commits\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"language\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"year\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"month\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"day\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"hour\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"minute\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"second\",\n",
      "    \"type\" : \"integer\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"ts\",\n",
      "    \"type\" : \"long\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary event_id (STRING);\n",
      "  optional binary event_type (STRING);\n",
      "  optional int96 created_at;\n",
      "  optional int64 repository_id;\n",
      "  optional binary repository_name (STRING);\n",
      "  optional binary repository_url (STRING);\n",
      "  optional int64 user_id;\n",
      "  optional binary user_name (STRING);\n",
      "  optional binary user_url (STRING);\n",
      "  optional binary user_avatar_url (STRING);\n",
      "  optional int64 org_id;\n",
      "  optional binary org_name (STRING);\n",
      "  optional binary org_url (STRING);\n",
      "  optional binary org_avatar_url (STRING);\n",
      "  optional int64 push_id;\n",
      "  optional int64 number_of_commits;\n",
      "  optional binary language (STRING);\n",
      "  optional int32 year;\n",
      "  optional int32 month;\n",
      "  optional int32 day;\n",
      "  optional int32 hour;\n",
      "  optional int32 minute;\n",
      "  optional int32 second;\n",
      "  optional int64 ts;\n",
      "}\n",
      "\n",
      "       \n",
      "24/02/10 20:05:38 INFO FileScanRDD: TID: 3 - Reading current file: path: s3a://gh-archive-data-raw/gh-archives/year=2023/month=01/day=01/hour=5/2023-01-01-5.json.gz, range: 0-66302326, partition values: [empty row], isDataPresent: false, eTag: null\n",
      "24/02/10 20:05:44 INFO FileOutputCommitter: Saved output of task 'attempt_202402102005382544477395443758455_0003_m_000000_3' to s3a://gh-archive-data-curated/parquet/gh_archive/year=2023/month=01/day=01/hour=5\n",
      "24/02/10 20:05:44 INFO SparkHadoopMapRedUtil: attempt_202402102005382544477395443758455_0003_m_000000_3: Committed. Elapsed time: 66 ms.\n",
      "24/02/10 20:05:44 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3035 bytes result sent to driver\n",
      "24/02/10 20:05:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 6256 ms on 931652ef741a (executor driver) (1/1)\n",
      "24/02/10 20:05:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/02/10 20:05:44 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 6.296 s\n",
      "24/02/10 20:05:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/02/10 20:05:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/02/10 20:05:44 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 6.298558 s\n",
      "24/02/10 20:05:44 INFO FileFormatWriter: Start to commit write Job 83cc354a-ffdd-46b9-9c28-ce9b65837041.\n",
      "24/02/10 20:05:44 INFO FileFormatWriter: Write Job 83cc354a-ffdd-46b9-9c28-ce9b65837041 committed. Elapsed time: 19 ms.\n",
      "24/02/10 20:05:44 INFO FileFormatWriter: Finished processing stats for write job 83cc354a-ffdd-46b9-9c28-ce9b65837041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/10 20:11:23 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply from 931652ef741a:34407 in 10000 milliseconds\n"
     ]
    }
   ],
   "source": [
    "# write the DataFrame to Storage\n",
    "\n",
    "main_df.write.format(\"parquet\"). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(\"s3a://gh-archive-data-curated/parquet/gh_archive/year=2023/month=01/day=01/hour=5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
